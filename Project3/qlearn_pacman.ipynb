{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3e30a1",
   "metadata": {},
   "source": [
    "# Projeto 3: Parte 2 - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7159e64d",
   "metadata": {},
   "source": [
    "**Estudantes:**\n",
    "\n",
    "* Giulia Silva Fazzi - RA: 225270\n",
    "* Muriel Guilherme Alves Mauch - RA: 225228"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc50253c",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c1746",
   "metadata": {},
   "source": [
    "Nesta segunda parte do trabalho 3, o objetivo do grupo foi resolver o problema do Pacman por meio da implementação de um algoritmo de Aprendizado por reforço. Para tal tarefa, nós escolhemos implementar o algoritmo **QLearn**.\n",
    "\n",
    "Como visto em aula, levamos em consideração algumas informações relevantes a natureza da solução, como por exemplo:\n",
    "\n",
    "* a cada passo t, o agente:\n",
    "    * executa uma ação $A_{t}$,\n",
    "    * recebe uma observação $O_{t}$ e,\n",
    "    * recebe uma recompensa $R_{t}$\n",
    "    \n",
    "Além do que, foi preciso definir nosso *Markov Decision Process (MDP)*, no qual o futuro é independente do passado, desde que dadas as informações do presente. Essa representação possui implicações importantíssimas, uma vez que nos ajuda na *complexidade computacional* e na *performance resultante*. \n",
    "\n",
    "Levamos em consideração também os componentes necessários de um agente de Aprendizado por Reforço, tais como:\n",
    "\n",
    "1. *Policy*: que representa o comportamento do agente;\n",
    "2. *Value function*: o quão bom é cada ação ou estado;\n",
    "3. Modelo: representação do ambiente pelo agente.\n",
    "\n",
    "Por último, é importante ressaltar que buscamos deixar explícitos, conforme requisitado na especificação do trabalho, as *recompensas, número de ações e score* por episódio.\n",
    "\n",
    "Destacamos também, que os passos para a configuração do ambiente estão explicitados no arquivo **README.md** mas serão adicionados aqui para a melhor compreensão dos corretores. Além disso, todos os códigos implementados por nós estão contidos nos arquivos **unused_agentes.py**, **agents.py** e **featureExtractor.py**, sendo que o primeiro foi utilizado como exercício para entendimento da base de código e não é relevante para este projeto. Já os dois últimos serão colocados aqui para a explicação da solução criada.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3c5b4",
   "metadata": {},
   "source": [
    "## Setup do ambiente e como executar o código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a725b",
   "metadata": {},
   "source": [
    "Para que pudessemos realizar a implementação foi necessário configurarmos nosso ambiente, portanto realizamos os seguintes passos:\n",
    "\n",
    "1. instalamos o python2,\n",
    "2. garantimos a instalação do tkinter,\n",
    "2. criamos um ambiente virtual (venv) e,\n",
    "3. executamos o código.\n",
    "\n",
    "Vamos explicitar os comandos abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02033c50",
   "metadata": {},
   "source": [
    "Para a criação do ambiente virtual, utilizamos o commando:\n",
    "\n",
    "` virtualenv --python=/usr/bin/python2.7 venv `\n",
    "\n",
    "Em seguida, ativamos a venv:\n",
    "\n",
    "` source venv/bin/activate `\n",
    "\n",
    "Instalamos o tkinter:\n",
    "\n",
    "` sudo apt-get install python-tk `\n",
    "\n",
    "E rodamos os algoritmos:\n",
    "\n",
    "` python2 pacman.py -p ApproximateQAgent -a extractor=FeatureExtractor -x 100 -n 110 -l smallClassic `\n",
    "\n",
    "` python2 pacman.py -p ApproximateQAgent -a extractor=FeatureExtractor -x 100 -n 110 -l mediumClassic `\n",
    "\n",
    "` python2 pacman.py -p ApproximateQAgent -a extractor=FeatureExtractor -x 100 -n 110 -l originalClassic `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3627b",
   "metadata": {},
   "source": [
    "## Implementação: QLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255b070",
   "metadata": {},
   "source": [
    "### Formulação da MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a007d",
   "metadata": {},
   "source": [
    "O primeiro passo, como já dito, foi a definição de nossa MDP com seus respectivos estados, ações e função de recompensa. Dessa forma, buscamos definir as funções para:\n",
    "\n",
    "1. conhecer os estados, \n",
    "2. definir o estado inicial,\n",
    "3. conhecer as ações,\n",
    "4. definir as transições e\n",
    "5. a função de recompensa.\n",
    "\n",
    "Ao analisarmos o código, percebemos que muitas dessas informações nos foram disponibilizadas pelo código base, e portanto as utilizamos em nosso favor, sendo assim:\n",
    "\n",
    "* os passos 1, 3 e 4 nos foram fornecidos pelo arquivo *pacman.py* por meio da classe *GameState*;\n",
    "* a definição do estado inicial foi feito por nós, no arquivo *agents.py*, na classe *ReinforcementAgent*;\n",
    "* a função recompensa foi definida por nós aproveitando o código fornecido, ou seja, utilizamos o próprio *score* como recompensa ao nosso agente.\n",
    "\n",
    "Dos componentes de um agente de Aprendizado por reforço, definimos:\n",
    "\n",
    "1. *policy*: pegamos as ações legais disponíveis, calculamos o *QValue* para cada uma e selecionamos o que nos fornece o melhor valor;\n",
    "2. *value function*: extraímos as características do ambiente no estado atual, como por exemplo as comidas disponíveis, a localização do pacman e dos fantasmas, assim como as paredes. Para cada uma das informações calculamos seu *QValue* e reajustamos os pesos;\n",
    "3. Modelo: o *QLearn* é *model-free*, portanto, não foi preciso nos preocuparmos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f8876",
   "metadata": {},
   "source": [
    "### Modelo de discretização, número de episódios de treino, critério de parada e demais parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491eee3",
   "metadata": {},
   "source": [
    "1. Não utilizamos um modelo de discretização;\n",
    "2. Foram 10000 episódios de treino;\n",
    "3. os critérios de parada foram: acabaram as \"comidas\" ou o pacman morreu;\n",
    "4. learning rate igual a 0.2, desconto de 0.8 e fator de exploração de 0.05.\n",
    "\n",
    "Relembramos mais uma vez que, para cada episódio, calculamos a recompensa (que foi o *score*) e o número de ações tomadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e2ccb",
   "metadata": {},
   "source": [
    "### Detalhando o código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c301ba",
   "metadata": {},
   "source": [
    "Como mencionamos, os códigos relevantes estão contidos nos arquivos *agents.py* e *featureExtractor.py*. Nesta seção, vamos discutir cada pedaço da implementação. Lembramos que, por costume de implementação os códigos foram desenvolvidos e comentados em inglês e manteremos esta estrutura aqui, mas faremos comentários adicionais em português, para manter o padrão do relatório. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3073d",
   "metadata": {},
   "source": [
    "Por facilidade de implementação, decidimos por uma estrutura multi-agente e implementamos no arquivo *agents.py* as seguintes classes: *ReinforcementAgent, QLearningAgent e ApproximateQAgent*. Já no arquivo *featureExtractors.py*, implementamos a classe de mesmo nome. Vamos mergulhar em cada uma delas agora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2714f",
   "metadata": {},
   "source": [
    "#### **agents.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c8e41",
   "metadata": {},
   "source": [
    "##### Classe ReinforcementAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d49735",
   "metadata": {},
   "source": [
    "Na classe *ReinforcementAgent*, definimos o agente que estima os *QValues* assim como as políticas por meio da experiência adquirida. Nela, definimos a maioria dos atributos principais. Esta classe recebe em seu construtor uma lista de ações disponíveis fornecida pelo *pacman.py*, assim como o número de episódios de treinamento e os parâmetros de aprendizagem *epsilon* (taxa de exploração), *alpha* (taxa de aprendizagem) e *gamma* (desconto). Todos eles possuem valores padrão, mas que são substituídos durante a execução. Além disso, no construtor são definidas outros atributos importantes, como o número de episódios ocorridos, o último estado e a última ação, assim como os *scores* acumulados de treino, teste e por episódio.\n",
    "\n",
    "```\n",
    "class ReinforcementAgent(Agent):\n",
    "    \"\"\"\n",
    "    The Reinforcement Agent estimates Q-Values (as well as policies) from experience\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions_available=None, num_training=10, epsilon=0.5, alpha=0.5, gamma=1):\n",
    "        \"\"\"\n",
    "        Instantiates the Reinforcement Agent\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            actions_available: function that takes a state and return the legal actions\n",
    "            alpha: learning rate\n",
    "            epsilon: exploration rate\n",
    "            gamma: discount factor\n",
    "            num_training: number of training episodes\n",
    "        \"\"\"\n",
    "        if actions_available is None:\n",
    "            actions_available = lambda state: state.getLegalActions()\n",
    "\n",
    "        self.episodes_so_far = 0\n",
    "\n",
    "        self.number_of_actions_taken = 0\n",
    "        self.actions_available = actions_available\n",
    "\n",
    "        self.num_training = int(num_training)\n",
    "        self.epsilon = float(epsilon)  # exploration rate\n",
    "        self.alpha = float(alpha)  # learning rate\n",
    "        self.discount = float(gamma)  # discount factor\n",
    "\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "        self.accum_train_score = 0.0\n",
    "        self.accum_test_score = 0.0\n",
    "        self.episode_score = 0.0\n",
    "```\n",
    "\n",
    "O método *update* é implementado pelas classes filhas. Já o método *getLegalActions* apenas retorna uma lista com as ações disponíveis para um dado estado.\n",
    "\n",
    "```\n",
    "    def update(self, state, action, next_state, reward):\n",
    "        pass  # not implemented here\n",
    "\n",
    "    def getLegalActions(self, state):\n",
    "        # Get the actions available for a given state\n",
    "        return self.actions_available(state)\n",
    "```\n",
    "\n",
    "O método *observeTransition* é chamado para informar o agente que uma transição foi observada, ou seja, foi tomada uma ação e necessitamos atualizar os atributos de acordo.\n",
    "\n",
    "```\n",
    "    def observeTransition(self, state, action, next_state, delta_reward):\n",
    "        \"\"\"\n",
    "            Called to inform the agent that a transition has been observed.\n",
    "            We will call self.update with the same arguments\n",
    "        \"\"\"\n",
    "        self.episode_score += delta_reward\n",
    "        self.update(state, action, next_state, delta_reward)\n",
    "```\n",
    "\n",
    "O método *startEpisode* é auto-explicativo. Ele inicia o episódio garantindo o que os atributos essênciais sejam inicializados corretamente.\n",
    "\n",
    "```\n",
    "    def startEpisode(self):\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.episode_score = 0.0\n",
    "        self.number_of_actions_taken = 0\n",
    "```\n",
    "\n",
    "E o método *stopEpisode* é o responsável por finalizar o episódio. Nele, calculamos o *score/reward* e o número de ações. Além disso atualizamos o *score* total. Caso seja o último episódio de treinamento, definimos a taxa de aprendizado e de exploração como 0. \n",
    "\n",
    "```\n",
    "    def stopEpisode(self):\n",
    "        print(\"Ending episode: {}\".format(self.episodes_so_far + 1))\n",
    "        print('Score: {}'.format(self.episode_score))\n",
    "        message = 'Number of actions taken: {}'.format(self.number_of_actions_taken)\n",
    "        print(message)\n",
    "        print(\"-\" * len(message))\n",
    "\n",
    "        if self.episodes_so_far < self.num_training:\n",
    "            self.accum_train_score += self.episode_score\n",
    "        else:\n",
    "            self.accum_test_score += self.episode_score\n",
    "        self.episodes_so_far += 1\n",
    "        if self.episodes_so_far >= self.num_training:\n",
    "            # now we remove the parameters\n",
    "            self.epsilon = 0.0\n",
    "            self.alpha = 0.0\n",
    "```\n",
    "\n",
    "O método *doAction* atualiza o contador de ações e define a ação e estados tomados.\n",
    "\n",
    "```\n",
    "    def doAction(self, state, action):\n",
    "        self.number_of_actions_taken += 1\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "```\n",
    "\n",
    "Já o método *observationFunction*, atualiza a recompensa assim que a ação mais recente é tomada, e chama o método *ObserveFunction* para atualizar os atributos do agente.\n",
    "\n",
    "```\n",
    "    def observationFunction(self, state):\n",
    "        # Called right after the last action\n",
    "        if not self.last_state is None:\n",
    "            reward = state.getScore() - self.last_state.getScore()\n",
    "            self.observeTransition(self.last_state, self.last_action, state, reward)\n",
    "        return state\n",
    "```\n",
    "\n",
    "Por fim, os métodos *registerInitialState* e *final* também são auto-explicativos. O primeiro chama o método *startEpisode* e o segundo finaliza o jogo, sendo que este último é chamado quando uma das condições de parada são atingidas.\n",
    "\n",
    "```\n",
    "    def registerInitialState(self, state):\n",
    "        self.startEpisode()\n",
    "        if self.episodes_so_far == 0:\n",
    "            message = \"Beginning {} episodes of Training\".format(self.num_training)\n",
    "            print('-' * len(message))\n",
    "            print(message)\n",
    "            print('-' * len(message))\n",
    "\n",
    "    def final(self, state):\n",
    "        # finishes the game\n",
    "        delta_reward = state.getScore() - self.last_state.getScore()\n",
    "        self.observeTransition(self.last_state, self.last_action, state, delta_reward)\n",
    "        self.stopEpisode()\n",
    "\n",
    "        if self.episodes_so_far == self.num_training:\n",
    "            msg = 'Finished training'\n",
    "            print(\"{}\\n{}\".format(msg, '-' * len(msg)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d865b0",
   "metadata": {},
   "source": [
    "##### Classe QLearningAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771bac1",
   "metadata": {},
   "source": [
    "Logo em seguida, implementamos a classe *QLearningAgent*, a qual herda da classe *ReinforcementAgent*. Nela, implementamos os métodos do nosso algoritmo de aprendizado. Em seu construtor apenas instanciamos a classe mãe e printamos os atributos.\n",
    "\n",
    "```\n",
    "class QLearningAgent(ReinforcementAgent):\n",
    "    \"\"\"\n",
    "      This is the Q-Learning Agent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **args):\n",
    "        ReinforcementAgent.__init__(self, **args)\n",
    "\n",
    "        self.q_values = util.Counter()\n",
    "        print(\"alpha/learning rate: {}\".format(self.alpha))\n",
    "        print(\"gamma/discount: {}\".format(self.discount))\n",
    "        print(\"epsilon/exploration: {}\".format(self.epsilon))\n",
    "```\n",
    "\n",
    "Em seu método *getQValue*, retornamos a tupla com o estado e ação.\n",
    "\n",
    "```\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "          Returns Q(state,action)\n",
    "          Should return 0.0 if we never seen a state or (state,action) tuple\n",
    "        \"\"\"\n",
    "        return self.q_values[(state, action)]\n",
    "```\n",
    "\n",
    "No método *getValue*, para cada combinação de ação válida para o estado disponível, calculamos o *QValue*, supra-citado.\n",
    "\n",
    "```\n",
    "    def getValue(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action) where the max is over legal actions. \n",
    "          Return 0.0 if final state\n",
    "        \"\"\"\n",
    "        possible_state_q_values = util.Counter()\n",
    "        for action in self.getLegalActions(state):\n",
    "            possible_state_q_values[action] = self.getQValue(state, action)\n",
    "\n",
    "        return possible_state_q_values[possible_state_q_values.argMax()]\n",
    "```\n",
    "\n",
    "Já o método *getPolicy* é um dos mais importantes de nossa solução. Nele, dado um estado calculamos a melhor ação a ser tomada. Caso seja a primeira execução, a ação será escolhida aleatoriamente,\n",
    "\n",
    "```\n",
    "    def getPolicy(self, state):\n",
    "        \"\"\"\n",
    "          Compute the best action to take in a state.  \n",
    "          Returns none if final state\n",
    "        \"\"\"\n",
    "        possible_state_q_values = util.Counter()\n",
    "        possible_actions = self.getLegalActions(state)\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        for action in possible_actions:\n",
    "            possible_state_q_values[action] = self.getQValue(state, action)\n",
    "\n",
    "        if possible_state_q_values.totalCount() == 0:\n",
    "            return random.choice(possible_actions)\n",
    "        else:\n",
    "            return possible_state_q_values.argMax()\n",
    "```\n",
    "\n",
    "O método *getAction* é utilizado para selecionar a melhor ação a ser tomada. Caso o cálculo dos pesos nos forneca uma taxa de exploração muito alta, uma ação é selecionada aleatoriamente, caso contrário, a melhor ação é selecionada por meio da chamada do método *getPolicy*.\n",
    "\n",
    "```\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "          Compute the action to take in the current state.  \n",
    "          With probability self.epsilon >> take a random action \n",
    "          otherwise >> take the best policy action \n",
    "          None if terminal state\n",
    "        \"\"\"\n",
    "        legal_actions = self.getLegalActions(state)\n",
    "        action = None\n",
    "        if len(legal_actions) > 0:\n",
    "            if util.flipCoin(self.epsilon):\n",
    "                action = random.choice(legal_actions)\n",
    "            else:\n",
    "                action = self.getPolicy(state)\n",
    "\n",
    "        return action\n",
    "```\n",
    "\n",
    "Por último, definimos o método *update*, o qual é chamado pela classe mãe para observar a tomada de uma ação em um dado estado. Nele, identificamos o próximo estado e calculamos a recompensa. A recompensa é calculada conforme vimos em aula, dada a função: \n",
    "\n",
    "\\begin{equation}\n",
    "Q(S_t,A_t) = Q(S_t,A_t) + alpha(R_t+1 + epsilon*Q(S_t+1,A') - Q(S_t,A_t))\n",
    "\\end{equation}\n",
    "\n",
    "Onde o *epsilon* é definido pela recompensa somada ao fator de desconto.\n",
    "\n",
    "```\n",
    "    def update(self, state, action, next_state, reward):\n",
    "        \"\"\"\n",
    "          The parent class calls this to observe a state = action => nextState and reward transition.\n",
    "          Here is where we update our Q-Value\n",
    "        \"\"\"\n",
    "        print(\"State: {}. Action: {}. NextState: {}. Reward: {}\".format(state, action, next_state, reward))\n",
    "        print(\"QVALUE: {}\".format(self.getQValue(state, action)))\n",
    "        print(\"VALUE: {}\".format(self.getValue(next_state)))\n",
    "        self.q_values[(state, action)] = self.getQValue(state, action) + self.alpha * (\n",
    "                reward + self.discount * self.getValue(next_state) - self.getQValue(state, action))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c1fa1",
   "metadata": {},
   "source": [
    "##### Classe ApproximateQAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6960da7c",
   "metadata": {},
   "source": [
    "Por fim, implementamos a classe *ApproximateQAgent*, a qual herda da classe *QLearningAgent*. Nessa, instanciamos nosso extrator de características e atualizamos os pesos para cada iteração. \n",
    "\n",
    "```\n",
    "class ApproximateQAgent(QLearningAgent):\n",
    "    \"\"\"\n",
    "       ApproximateQLearningAgent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, extractor='FeatureExtractor', epsilon=0.05, gamma=0.8, alpha=0.2, num_training=0, **args):\n",
    "        self.featExtractor = FeatureExtractor()\n",
    "\n",
    "        args['epsilon'] = epsilon\n",
    "        args['gamma'] = gamma\n",
    "        args['alpha'] = alpha\n",
    "        args['num_training'] = num_training\n",
    "        self.index = 0  # Pacman initial position\n",
    "        QLearningAgent.__init__(self, **args)\n",
    "        self.weights = util.Counter()\n",
    "```\n",
    "\n",
    "Instanciamos nela o método *getQValue*, o qual utiliza as características do ambiente e utiliza os valores das *features* com os pesos atualizados.\n",
    "\n",
    "```\n",
    "    def getQValue(self, state, action):\n",
    "        # return Q(state,action) = w * featureVector\n",
    "        q_value = 0.0\n",
    "        features = self.featExtractor.getFeatures(state, action)\n",
    "        for key in features.keys():\n",
    "            q_value += (self.weights[key] * features[key])\n",
    "        return q_value\n",
    "```\n",
    "\n",
    "O método *getAction* utiliza os métodos da classe mãe, a *QLearnAgent*, para tomar uma ação e atualizar os parâmetros.\n",
    "\n",
    "```\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "        Simply calls the getAction method of QLearningAgent and then\n",
    "        informs parent of action for Pacman.  Do not change or remove this\n",
    "        method.\n",
    "        \"\"\"\n",
    "        action = QLearningAgent.getAction(self, state)\n",
    "        self.doAction(state, action)\n",
    "        return action\n",
    "```\n",
    "\n",
    "Já o método *update* atualiza os pesos para cada transição, atualizando também o valor de Q. Nele utilizamos também a expressão mencionada no bloco acima, com algumas alterações, para reajuste dos pesos de cada uma das *features*.\n",
    "\n",
    "```\n",
    "    def update(self, state, action, next_state, reward):\n",
    "        # Should update your weights based on transition\n",
    "        # we get the features based on the current state and action, ignoring the past\n",
    "        # this allows us to have a huge performance advantage\n",
    "        features = self.featExtractor.getFeatures(state, action)\n",
    "        possible_state_q_values = []\n",
    "\n",
    "        for act in self.getLegalActions(state):\n",
    "            possible_state_q_values.append(self.getQValue(state, act))\n",
    "        for key in features.keys():\n",
    "            self.weights[key] += self.alpha * (reward + self.discount * (\n",
    "                    (1 - self.epsilon) * self.getValue(next_state) + (self.epsilon / len(possible_state_q_values)) * (\n",
    "                sum(possible_state_q_values))) - self.getQValue(state, action)) * features[key]\n",
    "\n",
    "```\n",
    "\n",
    "Por fim, o método *final* apenas finaliza o game, chamando o método da classe mãe.\n",
    "\n",
    "```\n",
    "    def final(self, state):\n",
    "        # called to end the game\n",
    "        QLearningAgent.final(self, state)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cec9c",
   "metadata": {},
   "source": [
    "#### featureExtractors.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953294e",
   "metadata": {},
   "source": [
    "A classe *featureExtractor* retorna as informações do ambiente para o agente. Como por exemplo, as comidas disponíveis, a posição do pacman e dos fantasmas, a posição das paredes, etc. Vamos explorar o código.\n",
    "\n",
    "**Relembramos que todas as características são independentes do passado e levam em consideração o estado atual apenas**\n",
    "\n",
    "O construtor é vazio e o primeiro método, chamado *is_food_close*, recebe uma posição futura do pacman, uma lista de \"comidas\", que é uma lista de posições, e a posição das paredes. Neste método, são identificadas as posições das comidas disponíveis, assim como as posições já visitadas. Enquanto houver comida disponível no ambiente, verificamos se a posição foi visitada, caso não tenha sido, atualizamos essa informação. Em seguida, se houver comida na posição mais próximas, a retornamos. Se não, procuramos nas posições adjacentes, utilizando as funções já disponibilizadas pelo game, como a *Actions.getLegalNeighbors*. Caso não haja mais comida, retornamos *None*.\n",
    "\n",
    "```\n",
    "    def is_food_close(self, pacman_future_location, food, walls):\n",
    "        food_available = [(pacman_future_location[0], pacman_future_location[1], 0)]\n",
    "        visited = set()\n",
    "\n",
    "        while food_available:\n",
    "            pacman_future_location_x, pacman_future_location_y, dist = food_available.pop(0)\n",
    "            if (pacman_future_location_x, pacman_future_location_y) in visited:\n",
    "                continue\n",
    "            visited.add((pacman_future_location_x, pacman_future_location_y))\n",
    "            # return if we find food at this position\n",
    "            if food[pacman_future_location_x][pacman_future_location_y]:\n",
    "                return dist\n",
    "            # if not, we will search the neighbours positions\n",
    "            nbrs = Actions.getLegalNeighbors((pacman_future_location_x, pacman_future_location_y), walls)\n",
    "            for nbr_x, nbr_y in nbrs:\n",
    "                food_available.append((nbr_x, nbr_y, dist + 1))\n",
    "        # return None if no more food is found\n",
    "        return None\n",
    "```\n",
    "\n",
    "A função *map_manhattan_distances* é utilizada para retornar a distância do pacman para os fantasmas, utilizando o método disponibilizado em *util.manhattanDistance*.\n",
    "\n",
    "```\n",
    "    def map_manhattan_distances(self, pacman_position, ghosts):\n",
    "        return map(lambda g: util.manhattanDistance(pacman_position, g.getPosition()), ghosts)\n",
    "```\n",
    "\n",
    "E por fim, o método *getFeatures* é utilizado, como vimos, pelo agente *ApproximateQAgent*. Ele fornece ao agente as informações de onde estão as comidas, as paredes, a posição do pacman e dos fantasmas.\n",
    "\n",
    "```\n",
    "    def getFeatures(self, state, action):\n",
    "        \"\"\"\n",
    "            extract the following information:\n",
    "                grid of food\n",
    "                wall locations and\n",
    "                get the ghost locations\n",
    "        \"\"\"\n",
    "        available_food = state.getFood()\n",
    "        walls_positions = state.getWalls()\n",
    "        ghosts_positions = state.getGhostPositions()\n",
    "\n",
    "        capsules_left = len(state.getCapsules())\n",
    "        scared_ghost = []\n",
    "        active_ghost = []\n",
    "\n",
    "        features = util.Counter()\n",
    "\n",
    "        for ghost in state.getGhostStates():\n",
    "            if not ghost.scaredTimer:\n",
    "                active_ghost.append(ghost)\n",
    "            else:\n",
    "                scared_ghost.append(ghost)\n",
    "\n",
    "        pacman_location = state.getPacmanPosition()\n",
    "\n",
    "        features[\"bias\"] = 1.0\n",
    "\n",
    "        # compute the location of pacman after he takes the action\n",
    "        x, y = state.getPacmanPosition()\n",
    "        dx, dy = Actions.directionToVector(action)\n",
    "        next_x, next_y = int(x + dx), int(y + dy)\n",
    "\n",
    "        # count the number of ghosts 1-step away\n",
    "        features[\"#-of-ghosts-1-step-away\"] = sum(\n",
    "            (next_x, next_y) in Actions.getLegalNeighbors(ghost, walls_positions) for ghost in ghosts_positions)\n",
    "\n",
    "        # if there is no danger of ghosts then add the food feature\n",
    "        if not features[\"#-of-ghosts-1-step-away\"] and available_food[next_x][next_y]:\n",
    "            features[\"eats-food\"] = 1.0\n",
    "\n",
    "        dist = self.is_food_close((next_x, next_y), available_food, walls_positions)\n",
    "        if dist is not None:\n",
    "            features[\"closest-food\"] = float(dist) / (walls_positions.width * walls_positions.height)\n",
    "\n",
    "        if scared_ghost:\n",
    "            distance_to_closest_scared_ghost = min(self.map_manhattan_distances(pacman_location, scared_ghost))\n",
    "            if active_ghost:\n",
    "                distance_to_closest_active_ghost = min(self.map_manhattan_distances(pacman_location, active_ghost))\n",
    "            else:\n",
    "                distance_to_closest_active_ghost = 10\n",
    "\n",
    "            features[\"capsules\"] = capsules_left\n",
    "\n",
    "            if distance_to_closest_scared_ghost <= 8 and distance_to_closest_active_ghost >= 2:\n",
    "                features[\"#-of-ghosts-1-step-away\"] = 0\n",
    "                features[\"eats-food\"] = 0.0\n",
    "\n",
    "        features.divideAll(10.0)\n",
    "        return features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1fc2e",
   "metadata": {},
   "source": [
    "## Execução e Resultados Obtidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b6ce2d",
   "metadata": {},
   "source": [
    "Como mencionado, treinamos nosso pacman por 10000 episódios, utilizando uma taxa de aprendizagem de 0.2, uma taxa de exploração de 0.05 e desconto de 0.8. O comando para a execução do código é o seguinte:\n",
    "\n",
    "`python2 pacman.py -p ApproximateQAgent -a extractor=FeatureExtractor -x 10000 -n 1000 -l smallClassic`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91874f62",
   "metadata": {},
   "source": [
    "Para o jogo usando o layout smallClassic, vemos que nosso agente apresenta o seguinte comportamento no treinamento:\n",
    "\n",
    "\n",
    "\n",
    "E no conjunto de teste, obtivemos os seguintes resultados:\n",
    "\n",
    "| Partida | Score | Nro de Ações | Resultado | \n",
    "|---------|-------|--------------|-----------|\n",
    "|    1    |       |              |           |\n",
    "|    2    |       |              |           |\n",
    "|    3    |       |              |           |\n",
    "|    4    |       |              |           |\n",
    "|    5    |       |              |           |\n",
    "|    6    |       |              |           |\n",
    "|    7    |       |              |           |\n",
    "|    8    |       |              |           |\n",
    "|    9    |       |              |           |\n",
    "|    10   |       |              |           |\n",
    "\n",
    "Podemos notar que a partir do jogo 71, há uma normalização nas vitórias e passamos a atingir sempre 9 vitórias dentre as 10 partidas. Resultados que se repetem nas partidas de teste, onde também conseguimos vencer 9/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "188a3c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha/learning rate: 0.2\n",
      "gamma/discount: 0.8\n",
      "epsilon/exploration: 0.05\n",
      "----------------------------------\n",
      "Beginning 100 episodes of Training\n",
      "----------------------------------\n",
      "Ending episode: 1\n",
      "Score: -293.0\n",
      "Number of actions taken: 23\n",
      "---------------------------\n",
      "Ending episode: 2\n",
      "Score: 33.0\n",
      "Number of actions taken: 47\n",
      "---------------------------\n",
      "Ending episode: 3\n",
      "Score: 1332.0\n",
      "Number of actions taken: 138\n",
      "----------------------------\n",
      "Ending episode: 4\n",
      "Score: 1536.0\n",
      "Number of actions taken: 134\n",
      "----------------------------\n",
      "Ending episode: 5\n",
      "Score: 1484.0\n",
      "Number of actions taken: 186\n",
      "----------------------------\n",
      "Ending episode: 6\n",
      "Score: 654.0\n",
      "Number of actions taken: 136\n",
      "----------------------------\n",
      "Ending episode: 7\n",
      "Score: 1928.0\n",
      "Number of actions taken: 142\n",
      "----------------------------\n",
      "Ending episode: 8\n",
      "Score: 292.0\n",
      "Number of actions taken: 98\n",
      "---------------------------\n",
      "Ending episode: 9\n",
      "Score: 1331.0\n",
      "Number of actions taken: 139\n",
      "----------------------------\n",
      "Ending episode: 10\n",
      "Score: 258.0\n",
      "Number of actions taken: 82\n",
      "---------------------------\n",
      "Ending episode: 11\n",
      "Score: 55.0\n",
      "Number of actions taken: 75\n",
      "---------------------------\n",
      "Ending episode: 12\n",
      "Score: 1525.0\n",
      "Number of actions taken: 145\n",
      "----------------------------\n",
      "Ending episode: 13\n",
      "Score: 39.0\n",
      "Number of actions taken: 81\n",
      "---------------------------\n",
      "Ending episode: 14\n",
      "Score: 449.0\n",
      "Number of actions taken: 141\n",
      "----------------------------\n",
      "Ending episode: 15\n",
      "Score: 1540.0\n",
      "Number of actions taken: 130\n",
      "----------------------------\n",
      "Ending episode: 16\n",
      "Score: 584.0\n",
      "Number of actions taken: 96\n",
      "---------------------------\n",
      "Ending episode: 17\n",
      "Score: 1526.0\n",
      "Number of actions taken: 144\n",
      "----------------------------\n",
      "Ending episode: 18\n",
      "Score: 1526.0\n",
      "Number of actions taken: 144\n",
      "----------------------------\n",
      "Ending episode: 19\n",
      "Score: 1329.0\n",
      "Number of actions taken: 141\n",
      "----------------------------\n",
      "Ending episode: 20\n",
      "Score: 1537.0\n",
      "Number of actions taken: 133\n",
      "----------------------------\n",
      "Ending episode: 21\n",
      "Score: 1518.0\n",
      "Number of actions taken: 152\n",
      "----------------------------\n",
      "Ending episode: 22\n",
      "Score: -336.0\n",
      "Number of actions taken: 26\n",
      "---------------------------\n",
      "Ending episode: 23\n",
      "Score: 1740.0\n",
      "Number of actions taken: 130\n",
      "----------------------------\n",
      "Ending episode: 24\n",
      "Score: 1726.0\n",
      "Number of actions taken: 144\n",
      "----------------------------\n",
      "Ending episode: 25\n",
      "Score: 479.0\n",
      "Number of actions taken: 121\n",
      "----------------------------\n",
      "Ending episode: 26\n",
      "Score: 1324.0\n",
      "Number of actions taken: 146\n",
      "----------------------------\n",
      "Ending episode: 27\n",
      "Score: 1541.0\n",
      "Number of actions taken: 129\n",
      "----------------------------\n",
      "Ending episode: 28\n",
      "Score: 1532.0\n",
      "Number of actions taken: 138\n",
      "----------------------------\n",
      "Ending episode: 29\n",
      "Score: 1330.0\n",
      "Number of actions taken: 140\n",
      "----------------------------\n",
      "Ending episode: 30\n",
      "Score: -367.0\n",
      "Number of actions taken: 17\n",
      "---------------------------\n",
      "Ending episode: 31\n",
      "Score: 98.0\n",
      "Number of actions taken: 52\n",
      "---------------------------\n",
      "Ending episode: 32\n",
      "Score: 1504.0\n",
      "Number of actions taken: 166\n",
      "----------------------------\n",
      "Ending episode: 33\n",
      "Score: -350.0\n",
      "Number of actions taken: 20\n",
      "---------------------------\n",
      "Ending episode: 34\n",
      "Score: 2128.0\n",
      "Number of actions taken: 142\n",
      "----------------------------\n",
      "Ending episode: 35\n",
      "Score: 461.0\n",
      "Number of actions taken: 129\n",
      "----------------------------\n",
      "Ending episode: 36\n",
      "Score: -20.0\n",
      "Number of actions taken: 40\n",
      "---------------------------\n",
      "Ending episode: 37\n",
      "Score: -325.0\n",
      "Number of actions taken: 25\n",
      "---------------------------\n",
      "Ending episode: 38\n",
      "Score: 291.0\n",
      "Number of actions taken: 139\n",
      "----------------------------\n",
      "Ending episode: 39\n",
      "Score: 253.0\n",
      "Number of actions taken: 97\n",
      "---------------------------\n",
      "Ending episode: 40\n",
      "Score: 1518.0\n",
      "Number of actions taken: 152\n",
      "----------------------------\n",
      "Ending episode: 41\n",
      "Score: 1726.0\n",
      "Number of actions taken: 144\n",
      "----------------------------\n",
      "Ending episode: 42\n",
      "Score: 856.0\n",
      "Number of actions taken: 124\n",
      "----------------------------\n",
      "Ending episode: 43\n",
      "Score: 1725.0\n",
      "Number of actions taken: 145\n",
      "----------------------------\n",
      "Ending episode: 44\n",
      "Score: 1327.0\n",
      "Number of actions taken: 143\n",
      "----------------------------\n",
      "Ending episode: 45\n",
      "Score: 1516.0\n",
      "Number of actions taken: 154\n",
      "----------------------------\n",
      "Ending episode: 46\n",
      "Score: 1320.0\n",
      "Number of actions taken: 150\n",
      "----------------------------\n",
      "Ending episode: 47\n",
      "Score: 196.0\n",
      "Number of actions taken: 84\n",
      "---------------------------\n",
      "Ending episode: 48\n",
      "Score: 708.0\n",
      "Number of actions taken: 152\n",
      "----------------------------\n",
      "Ending episode: 49\n",
      "Score: 1537.0\n",
      "Number of actions taken: 133\n",
      "----------------------------\n",
      "Ending episode: 50\n",
      "Score: 1721.0\n",
      "Number of actions taken: 149\n",
      "----------------------------\n",
      "Ending episode: 51\n",
      "Score: 1526.0\n",
      "Number of actions taken: 144\n",
      "----------------------------\n",
      "Ending episode: 52\n",
      "Score: -43.0\n",
      "Number of actions taken: 63\n",
      "---------------------------\n",
      "Ending episode: 53\n",
      "Score: 1535.0\n",
      "Number of actions taken: 135\n",
      "----------------------------\n",
      "Ending episode: 54\n",
      "Score: 138.0\n",
      "Number of actions taken: 72\n",
      "---------------------------\n",
      "Ending episode: 55\n",
      "Score: 1526.0\n",
      "Number of actions taken: 144\n",
      "----------------------------\n",
      "Ending episode: 56\n",
      "Score: 427.0\n",
      "Number of actions taken: 113\n",
      "----------------------------\n",
      "Ending episode: 57\n",
      "Score: 134.0\n",
      "Number of actions taken: 56\n",
      "---------------------------\n",
      "Ending episode: 58\n",
      "Score: 439.0\n",
      "Number of actions taken: 151\n",
      "----------------------------\n",
      "Ending episode: 59\n",
      "Score: 1735.0\n",
      "Number of actions taken: 135\n",
      "----------------------------\n",
      "Ending episode: 60\n",
      "Score: 1524.0\n",
      "Number of actions taken: 146\n",
      "----------------------------\n",
      "Ending episode: 61\n",
      "Score: 452.0\n",
      "Number of actions taken: 138\n",
      "----------------------------\n",
      "Ending episode: 62\n",
      "Score: 1310.0\n",
      "Number of actions taken: 160\n",
      "----------------------------\n",
      "Ending episode: 63\n",
      "Score: 248.0\n",
      "Number of actions taken: 82\n",
      "---------------------------\n",
      "Ending episode: 64\n",
      "Score: 84.0\n",
      "Number of actions taken: 46\n",
      "---------------------------\n",
      "Ending episode: 65\n",
      "Score: 569.0\n",
      "Number of actions taken: 101\n",
      "----------------------------\n",
      "Ending episode: 66\n",
      "Score: 1710.0\n",
      "Number of actions taken: 160\n",
      "----------------------------\n",
      "Ending episode: 67\n",
      "Score: 181.0\n",
      "Number of actions taken: 69\n",
      "---------------------------\n",
      "Ending episode: 68\n",
      "Score: 1319.0\n",
      "Number of actions taken: 151\n",
      "----------------------------\n",
      "Ending episode: 69\n",
      "Score: 1542.0\n",
      "Number of actions taken: 128\n",
      "----------------------------\n",
      "Ending episode: 70\n",
      "Score: 1332.0\n",
      "Number of actions taken: 138\n",
      "----------------------------\n",
      "Ending episode: 71\n",
      "Score: 1540.0\n",
      "Number of actions taken: 130\n",
      "----------------------------\n",
      "Ending episode: 72\n",
      "Score: 1327.0\n",
      "Number of actions taken: 143\n",
      "----------------------------\n",
      "Ending episode: 73\n",
      "Score: 1724.0\n",
      "Number of actions taken: 146\n",
      "----------------------------\n",
      "Ending episode: 74\n",
      "Score: 256.0\n",
      "Number of actions taken: 74\n",
      "---------------------------\n",
      "Ending episode: 75\n",
      "Score: 1732.0\n",
      "Number of actions taken: 138\n",
      "----------------------------\n",
      "Ending episode: 76\n",
      "Score: 1528.0\n",
      "Number of actions taken: 142\n",
      "----------------------------\n",
      "Ending episode: 77\n",
      "Score: 1529.0\n",
      "Number of actions taken: 141\n",
      "----------------------------\n",
      "Ending episode: 78\n",
      "Score: 1931.0\n",
      "Number of actions taken: 139\n",
      "----------------------------\n",
      "Ending episode: 79\n",
      "Score: 192.0\n",
      "Number of actions taken: 68\n",
      "---------------------------\n",
      "Ending episode: 80\n",
      "Score: 1523.0\n",
      "Number of actions taken: 147\n",
      "----------------------------\n",
      "Ending episode: 81\n",
      "Score: 1927.0\n",
      "Number of actions taken: 143\n",
      "----------------------------\n",
      "Ending episode: 82\n",
      "Score: 1527.0\n",
      "Number of actions taken: 143\n",
      "----------------------------\n",
      "Ending episode: 83\n",
      "Score: 1527.0\n",
      "Number of actions taken: 143\n",
      "----------------------------\n",
      "Ending episode: 84\n",
      "Score: 1927.0\n",
      "Number of actions taken: 143\n",
      "----------------------------\n",
      "Ending episode: 85\n",
      "Score: 135.0\n",
      "Number of actions taken: 55\n",
      "---------------------------\n",
      "Ending episode: 86\n",
      "Score: 858.0\n",
      "Number of actions taken: 132\n",
      "----------------------------\n",
      "Ending episode: 87\n",
      "Score: -158.0\n",
      "Number of actions taken: 48\n",
      "---------------------------\n",
      "Ending episode: 88\n",
      "Score: 86.0\n",
      "Number of actions taken: 94\n",
      "---------------------------\n",
      "Ending episode: 89\n",
      "Score: 1730.0\n",
      "Number of actions taken: 140\n",
      "----------------------------\n",
      "Ending episode: 90\n",
      "Score: 138.0\n",
      "Number of actions taken: 62\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending episode: 91\n",
      "Score: 506.0\n",
      "Number of actions taken: 144\n",
      "----------------------------\n",
      "Ending episode: 92\n",
      "Score: 95.0\n",
      "Number of actions taken: 95\n",
      "---------------------------\n",
      "Ending episode: 93\n",
      "Score: -52.0\n",
      "Number of actions taken: 32\n",
      "---------------------------\n",
      "Ending episode: 94\n",
      "Score: 131.0\n",
      "Number of actions taken: 59\n",
      "---------------------------\n",
      "Ending episode: 95\n",
      "Score: -393.0\n",
      "Number of actions taken: 13\n",
      "---------------------------\n",
      "Ending episode: 96\n",
      "Score: 1928.0\n",
      "Number of actions taken: 142\n",
      "----------------------------\n",
      "Ending episode: 97\n",
      "Score: 186.0\n",
      "Number of actions taken: 94\n",
      "---------------------------\n",
      "Ending episode: 98\n",
      "Score: 1524.0\n",
      "Number of actions taken: 146\n",
      "----------------------------\n",
      "Ending episode: 99\n",
      "Score: 172.0\n",
      "Number of actions taken: 68\n",
      "---------------------------\n",
      "Ending episode: 100\n",
      "Score: -21.0\n",
      "Number of actions taken: 61\n",
      "---------------------------\n",
      "Finished training\n",
      "-----------------\n",
      "Pacman emerges victorious! Score: 1329\n",
      "Ending episode: 101\n",
      "Score: 1329.0\n",
      "Number of actions taken: 141\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 1544\n",
      "Ending episode: 102\n",
      "Score: 1544.0\n",
      "Number of actions taken: 126\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 1328\n",
      "Ending episode: 103\n",
      "Score: 1328.0\n",
      "Number of actions taken: 142\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 1731\n",
      "Ending episode: 104\n",
      "Score: 1731.0\n",
      "Number of actions taken: 139\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 1512\n",
      "Ending episode: 105\n",
      "Score: 1512.0\n",
      "Number of actions taken: 158\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 1725\n",
      "Ending episode: 106\n",
      "Score: 1725.0\n",
      "Number of actions taken: 145\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 1331\n",
      "Ending episode: 107\n",
      "Score: 1331.0\n",
      "Number of actions taken: 139\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 1330\n",
      "Ending episode: 108\n",
      "Score: 1330.0\n",
      "Number of actions taken: 140\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 1930\n",
      "Ending episode: 109\n",
      "Score: 1930.0\n",
      "Number of actions taken: 140\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 1523\n",
      "Ending episode: 110\n",
      "Score: 1523.0\n",
      "Number of actions taken: 147\n",
      "----------------------------\n",
      "Average Score: 1528.3\n",
      "Scores:        1329, 1544, 1328, 1731, 1512, 1725, 1331, 1330, 1930, 1523\n",
      "Win Rate:      10/10 (1.00)\n",
      "Record:        Win, Win, Win, Win, Win, Win, Win, Win, Win, Win\n"
     ]
    }
   ],
   "source": [
    "!python2 pacman.py -p ApproximateQAgent -a extractor=FeatureExtractor -x 100 -n 110 -l mediumClassic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699ff5b",
   "metadata": {},
   "source": [
    "Para o jogo usando o layout mediumClassic, podemos observar a seguinte performance:\n",
    "\n",
    "| Total de Partidas | Vitórias | Derrotas |\n",
    "|-------------------|----------|----------|\n",
    "|      0 - 10       |     5    |     5    |\n",
    "|     11 - 20       |     6    |     4    |\n",
    "|     21 - 30       |     7    |     3    |\n",
    "|     31 - 40       |     3    |     7    |\n",
    "|     41 - 50       |     7    |     3    |\n",
    "|     51 - 60       |     5    |     5    |\n",
    "|     61 - 70       |     5    |     5    |\n",
    "|     71 - 80       |     8    |     2    |\n",
    "|     81 - 90       |     5    |     5    |\n",
    "|     91 - 100      |     2    |     8    |\n",
    "\n",
    "Já para este layout, não fica claro uma convergência nos resultados. No entanto, nas partidas de teste, obtivemos 10 vitórias em 10 partidas.\n",
    "\n",
    "Average Score: 1528.3\n",
    "\n",
    "Win Rate:      10/10 (1.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "255afe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha/learning rate: 0.2\n",
      "gamma/discount: 0.8\n",
      "epsilon/exploration: 0.05\n",
      "----------------------------------\n",
      "Beginning 100 episodes of Training\n",
      "----------------------------------\n",
      "Ending episode: 1\n",
      "Score: 93.0\n",
      "Number of actions taken: 87\n",
      "---------------------------\n",
      "Ending episode: 2\n",
      "Score: 377.0\n",
      "Number of actions taken: 133\n",
      "----------------------------\n",
      "Ending episode: 3\n",
      "Score: 2850.0\n",
      "Number of actions taken: 340\n",
      "----------------------------\n",
      "Ending episode: 4\n",
      "Score: 2821.0\n",
      "Number of actions taken: 369\n",
      "----------------------------\n",
      "Ending episode: 5\n",
      "Score: 2474.0\n",
      "Number of actions taken: 316\n",
      "----------------------------\n",
      "Ending episode: 6\n",
      "Score: 2431.0\n",
      "Number of actions taken: 359\n",
      "----------------------------\n",
      "Ending episode: 7\n",
      "Score: 3047.0\n",
      "Number of actions taken: 343\n",
      "----------------------------\n",
      "Ending episode: 8\n",
      "Score: 784.0\n",
      "Number of actions taken: 136\n",
      "----------------------------\n",
      "Ending episode: 9\n",
      "Score: 1788.0\n",
      "Number of actions taken: 332\n",
      "----------------------------\n",
      "Ending episode: 10\n",
      "Score: 3062.0\n",
      "Number of actions taken: 328\n",
      "----------------------------\n",
      "Ending episode: 11\n",
      "Score: 2478.0\n",
      "Number of actions taken: 312\n",
      "----------------------------\n",
      "Ending episode: 12\n",
      "Score: 1245.0\n",
      "Number of actions taken: 225\n",
      "----------------------------\n",
      "Ending episode: 13\n",
      "Score: 1173.0\n",
      "Number of actions taken: 237\n",
      "----------------------------\n",
      "Ending episode: 14\n",
      "Score: 828.0\n",
      "Number of actions taken: 142\n",
      "----------------------------\n",
      "Ending episode: 15\n",
      "Score: 1930.0\n",
      "Number of actions taken: 310\n",
      "----------------------------\n",
      "Ending episode: 16\n",
      "Score: 1518.0\n",
      "Number of actions taken: 322\n",
      "----------------------------\n",
      "Ending episode: 17\n",
      "Score: -269.0\n",
      "Number of actions taken: 29\n",
      "---------------------------\n",
      "Ending episode: 18\n",
      "Score: 2877.0\n",
      "Number of actions taken: 313\n",
      "----------------------------\n",
      "Ending episode: 19\n",
      "Score: 2080.0\n",
      "Number of actions taken: 320\n",
      "----------------------------\n",
      "Ending episode: 20\n",
      "Score: -99.0\n",
      "Number of actions taken: 49\n",
      "---------------------------\n",
      "Ending episode: 21\n",
      "Score: 1618.0\n",
      "Number of actions taken: 312\n",
      "----------------------------\n",
      "Ending episode: 22\n",
      "Score: 1473.0\n",
      "Number of actions taken: 277\n",
      "----------------------------\n",
      "Ending episode: 23\n",
      "Score: 1374.0\n",
      "Number of actions taken: 306\n",
      "----------------------------\n",
      "Ending episode: 24\n",
      "Score: 2644.0\n",
      "Number of actions taken: 346\n",
      "----------------------------\n",
      "Ending episode: 25\n",
      "Score: 2664.0\n",
      "Number of actions taken: 326\n",
      "----------------------------\n",
      "Ending episode: 26\n",
      "Score: 1555.0\n",
      "Number of actions taken: 365\n",
      "----------------------------\n",
      "Ending episode: 27\n",
      "Score: 786.0\n",
      "Number of actions taken: 164\n",
      "----------------------------\n",
      "Ending episode: 28\n",
      "Score: 2457.0\n",
      "Number of actions taken: 333\n",
      "----------------------------\n",
      "Ending episode: 29\n",
      "Score: 2678.0\n",
      "Number of actions taken: 312\n",
      "----------------------------\n",
      "Ending episode: 30\n",
      "Score: 2446.0\n",
      "Number of actions taken: 344\n",
      "----------------------------\n",
      "Ending episode: 31\n",
      "Score: 2441.0\n",
      "Number of actions taken: 349\n",
      "----------------------------\n",
      "Ending episode: 32\n",
      "Score: 1473.0\n",
      "Number of actions taken: 307\n",
      "----------------------------\n",
      "Ending episode: 33\n",
      "Score: 1621.0\n",
      "Number of actions taken: 269\n",
      "----------------------------\n",
      "Ending episode: 34\n",
      "Score: 584.0\n",
      "Number of actions taken: 126\n",
      "----------------------------\n",
      "Ending episode: 35\n",
      "Score: 541.0\n",
      "Number of actions taken: 149\n",
      "----------------------------\n",
      "Ending episode: 36\n",
      "Score: 3462.0\n",
      "Number of actions taken: 328\n",
      "----------------------------\n",
      "Ending episode: 37\n",
      "Score: 2644.0\n",
      "Number of actions taken: 346\n",
      "----------------------------\n",
      "Ending episode: 38\n",
      "Score: 1610.0\n",
      "Number of actions taken: 350\n",
      "----------------------------\n",
      "Ending episode: 39\n",
      "Score: 2655.0\n",
      "Number of actions taken: 335\n",
      "----------------------------\n",
      "Ending episode: 40\n",
      "Score: 420.0\n",
      "Number of actions taken: 150\n",
      "----------------------------\n",
      "Ending episode: 41\n",
      "Score: 3026.0\n",
      "Number of actions taken: 364\n",
      "----------------------------\n",
      "Ending episode: 42\n",
      "Score: 52.0\n",
      "Number of actions taken: 78\n",
      "---------------------------\n",
      "Ending episode: 43\n",
      "Score: 421.0\n",
      "Number of actions taken: 149\n",
      "----------------------------\n",
      "Ending episode: 44\n",
      "Score: 906.0\n",
      "Number of actions taken: 154\n",
      "----------------------------\n",
      "Ending episode: 45\n",
      "Score: 873.0\n",
      "Number of actions taken: 177\n",
      "----------------------------\n",
      "Ending episode: 46\n",
      "Score: 229.0\n",
      "Number of actions taken: 101\n",
      "----------------------------\n",
      "Ending episode: 47\n",
      "Score: 2681.0\n",
      "Number of actions taken: 309\n",
      "----------------------------\n",
      "Ending episode: 48\n",
      "Score: 1681.0\n",
      "Number of actions taken: 229\n",
      "----------------------------\n",
      "Ending episode: 49\n",
      "Score: 406.0\n",
      "Number of actions taken: 134\n",
      "----------------------------\n",
      "Ending episode: 50\n",
      "Score: 2632.0\n",
      "Number of actions taken: 358\n",
      "----------------------------\n",
      "Ending episode: 51\n",
      "Score: 844.0\n",
      "Number of actions taken: 206\n",
      "----------------------------\n",
      "Ending episode: 52\n",
      "Score: 1607.0\n",
      "Number of actions taken: 263\n",
      "----------------------------\n",
      "Ending episode: 53\n",
      "Score: 1620.0\n",
      "Number of actions taken: 350\n",
      "----------------------------\n",
      "Ending episode: 54\n",
      "Score: 2638.0\n",
      "Number of actions taken: 352\n",
      "----------------------------\n",
      "Ending episode: 55\n",
      "Score: 2631.0\n",
      "Number of actions taken: 359\n",
      "----------------------------\n",
      "Ending episode: 56\n",
      "Score: 2873.0\n",
      "Number of actions taken: 317\n",
      "----------------------------\n",
      "Ending episode: 57\n",
      "Score: 2681.0\n",
      "Number of actions taken: 309\n",
      "----------------------------\n",
      "Ending episode: 58\n",
      "Score: 1693.0\n",
      "Number of actions taken: 237\n",
      "----------------------------\n",
      "Ending episode: 59\n",
      "Score: 570.0\n",
      "Number of actions taken: 170\n",
      "----------------------------\n",
      "Ending episode: 60\n",
      "Score: 2640.0\n",
      "Number of actions taken: 350\n",
      "----------------------------\n",
      "Ending episode: 61\n",
      "Score: 427.0\n",
      "Number of actions taken: 163\n",
      "----------------------------\n",
      "Ending episode: 62\n",
      "Score: 1605.0\n",
      "Number of actions taken: 355\n",
      "----------------------------\n",
      "Ending episode: 63\n",
      "Score: 3071.0\n",
      "Number of actions taken: 319\n",
      "----------------------------\n",
      "Ending episode: 64\n",
      "Score: 2850.0\n",
      "Number of actions taken: 340\n",
      "----------------------------\n",
      "Ending episode: 65\n",
      "Score: 2449.0\n",
      "Number of actions taken: 341\n",
      "----------------------------\n",
      "Ending episode: 66\n",
      "Score: 3048.0\n",
      "Number of actions taken: 342\n",
      "----------------------------\n",
      "Ending episode: 67\n",
      "Score: 662.0\n",
      "Number of actions taken: 168\n",
      "----------------------------\n",
      "Ending episode: 68\n",
      "Score: 3066.0\n",
      "Number of actions taken: 324\n",
      "----------------------------\n",
      "Ending episode: 69\n",
      "Score: 2874.0\n",
      "Number of actions taken: 316\n",
      "----------------------------\n",
      "Ending episode: 70\n",
      "Score: 876.0\n",
      "Number of actions taken: 154\n",
      "----------------------------\n",
      "Ending episode: 71\n",
      "Score: 653.0\n",
      "Number of actions taken: 177\n",
      "----------------------------\n",
      "Ending episode: 72\n",
      "Score: 2873.0\n",
      "Number of actions taken: 317\n",
      "----------------------------\n",
      "Ending episode: 73\n",
      "Score: 405.0\n",
      "Number of actions taken: 155\n",
      "----------------------------\n",
      "Ending episode: 74\n",
      "Score: 73.0\n",
      "Number of actions taken: 87\n",
      "---------------------------\n",
      "Ending episode: 75\n",
      "Score: 108.0\n",
      "Number of actions taken: 92\n",
      "---------------------------\n",
      "Ending episode: 76\n",
      "Score: 1103.0\n",
      "Number of actions taken: 217\n",
      "----------------------------\n",
      "Ending episode: 77\n",
      "Score: 2446.0\n",
      "Number of actions taken: 344\n",
      "----------------------------\n",
      "Ending episode: 78\n",
      "Score: 73.0\n",
      "Number of actions taken: 77\n",
      "---------------------------\n",
      "Ending episode: 79\n",
      "Score: 3027.0\n",
      "Number of actions taken: 363\n",
      "----------------------------\n",
      "Ending episode: 80\n",
      "Score: 506.0\n",
      "Number of actions taken: 134\n",
      "----------------------------\n",
      "Ending episode: 81\n",
      "Score: 75.0\n",
      "Number of actions taken: 95\n",
      "---------------------------\n",
      "Ending episode: 82\n",
      "Score: 3064.0\n",
      "Number of actions taken: 326\n",
      "----------------------------\n",
      "Ending episode: 83\n",
      "Score: 296.0\n",
      "Number of actions taken: 104\n",
      "----------------------------\n",
      "Ending episode: 84\n",
      "Score: 476.0\n",
      "Number of actions taken: 134\n",
      "----------------------------\n",
      "Ending episode: 85\n",
      "Score: 3058.0\n",
      "Number of actions taken: 332\n",
      "----------------------------\n",
      "Ending episode: 86\n",
      "Score: 2392.0\n",
      "Number of actions taken: 318\n",
      "----------------------------\n",
      "Ending episode: 87\n",
      "Score: 619.0\n",
      "Number of actions taken: 141\n",
      "----------------------------\n",
      "Ending episode: 88\n",
      "Score: 678.0\n",
      "Number of actions taken: 152\n",
      "----------------------------\n",
      "Ending episode: 89\n",
      "Score: 1821.0\n",
      "Number of actions taken: 299\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending episode: 90\n",
      "Score: 2660.0\n",
      "Number of actions taken: 330\n",
      "----------------------------\n",
      "Ending episode: 91\n",
      "Score: 2622.0\n",
      "Number of actions taken: 368\n",
      "----------------------------\n",
      "Ending episode: 92\n",
      "Score: 661.0\n",
      "Number of actions taken: 179\n",
      "----------------------------\n",
      "Ending episode: 93\n",
      "Score: 613.0\n",
      "Number of actions taken: 147\n",
      "----------------------------\n",
      "Ending episode: 94\n",
      "Score: 2871.0\n",
      "Number of actions taken: 319\n",
      "----------------------------\n",
      "Ending episode: 95\n",
      "Score: 861.0\n",
      "Number of actions taken: 149\n",
      "----------------------------\n",
      "Ending episode: 96\n",
      "Score: 1350.0\n",
      "Number of actions taken: 260\n",
      "----------------------------\n",
      "Ending episode: 97\n",
      "Score: 1345.0\n",
      "Number of actions taken: 255\n",
      "----------------------------\n",
      "Ending episode: 98\n",
      "Score: 745.0\n",
      "Number of actions taken: 165\n",
      "----------------------------\n",
      "Ending episode: 99\n",
      "Score: 1227.0\n",
      "Number of actions taken: 213\n",
      "----------------------------\n",
      "Ending episode: 100\n",
      "Score: 2859.0\n",
      "Number of actions taken: 331\n",
      "----------------------------\n",
      "Finished training\n",
      "-----------------\n",
      "Pacman emerges victorious! Score: 2690\n",
      "Ending episode: 101\n",
      "Score: 2690.0\n",
      "Number of actions taken: 300\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 2460\n",
      "Ending episode: 102\n",
      "Score: 2460.0\n",
      "Number of actions taken: 330\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 2689\n",
      "Ending episode: 103\n",
      "Score: 2689.0\n",
      "Number of actions taken: 301\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 2860\n",
      "Ending episode: 104\n",
      "Score: 2860.0\n",
      "Number of actions taken: 330\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 3286\n",
      "Ending episode: 105\n",
      "Score: 3286.0\n",
      "Number of actions taken: 304\n",
      "----------------------------\n",
      "Pacman died! Score: 63\n",
      "Ending episode: 106\n",
      "Score: 63.0\n",
      "Number of actions taken: 77\n",
      "---------------------------\n",
      "Pacman emerges victorious! Score: 3261\n",
      "Ending episode: 107\n",
      "Score: 3261.0\n",
      "Number of actions taken: 329\n",
      "----------------------------\n",
      "Pacman died! Score: 1960\n",
      "Ending episode: 108\n",
      "Score: 1960.0\n",
      "Number of actions taken: 300\n",
      "----------------------------\n",
      "Pacman died! Score: 573\n",
      "Ending episode: 109\n",
      "Score: 573.0\n",
      "Number of actions taken: 177\n",
      "----------------------------\n",
      "Pacman emerges victorious! Score: 2485\n",
      "Ending episode: 110\n",
      "Score: 2485.0\n",
      "Number of actions taken: 305\n",
      "----------------------------\n",
      "Average Score: 2232.7\n",
      "Scores:        2690, 2460, 2689, 2860, 3286, 63, 3261, 1960, 573, 2485\n",
      "Win Rate:      7/10 (0.70)\n",
      "Record:        Win, Win, Win, Win, Win, Loss, Win, Loss, Loss, Win\n"
     ]
    }
   ],
   "source": [
    "!python2 pacman.py -p ApproximateQAgent -a extractor=FeatureExtractor -x 100 -n 110 -l originalClassic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2cc45",
   "metadata": {},
   "source": [
    "Para o jogo usando o layout mediumClassic, podemos observar a seguinte performance:\n",
    "\n",
    "| Total de Partidas | Vitórias | Derrotas |\n",
    "|-------------------|----------|----------|\n",
    "|      0 - 10       |     6    |     4    |\n",
    "|     11 - 20       |     3    |     7    |\n",
    "|     21 - 30       |     5    |     5    |\n",
    "|     31 - 40       |     4    |     6    |\n",
    "|     41 - 50       |     3    |     7    |\n",
    "|     51 - 60       |     5    |     5    |\n",
    "|     61 - 70       |     6    |     4    |\n",
    "|     71 - 80       |     3    |     7    |\n",
    "|     81 - 90       |     4    |     6    |\n",
    "|     91 - 100      |     3    |     7    |\n",
    "\n",
    "Neste último layout também notamos a ausência de convergência. Mas, da mesma forma que no caso anterior, obtemos um resultado razoável nas partidas de teste, obtendo 7 vitórias em 10 partidas.\n",
    "\n",
    "Average Score: 2232.7\n",
    "\n",
    "Win Rate:      7/10 (0.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afda893",
   "metadata": {},
   "source": [
    "Para os dois últimos layouts, que são maiores, embora obtemos um resultado satisfatório nos testes, notamos que apenas 100 episódios de treino não são suficientes para a convergência dos resultados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
